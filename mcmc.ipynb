{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/probabilistic_python/blob/master/mcmc.ipynb)\n",
    "\n",
    "# MCMC in PyMC\n",
    "\n",
    "Though it provides several tools for Bayesian inference, PyMC's core business is using Markov chain Monte Carlo to fit virtually any probability model. This involves the assignment and coordination of a suite of **step methods**, each of which is responsible for updating one or more variables. \n",
    "\n",
    "Let's take a closer look at MCMC sampling in PyMC, motivating it with a new example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aesara.tensor as at\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import seaborn as sns\n",
    "\n",
    "random_seed = 42\n",
    "DATA_URL = \"https://raw.githubusercontent.com/fonnesbeck/probabilistic_python/master/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Six Nations Rugby Matchups\n",
    "\n",
    "The Six Nations Championship is one of the world's great rugby tournaments, with the six top rugby nations in Europe competing each year: England, Wales, Scotland, Ireland, France, and Italy.\n",
    "\n",
    "![](assets/six-nations.png)\n",
    "\n",
    "We may be interested in building a model that predicts the outcome of matches between any two teams, based on past performances. We know that Ireland is better than Italy, but how much better?\n",
    "\n",
    "One approach is to build a **latent variable model** that describes the strength of a team based on their propensity for scoring or givin up points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rugby_data = pd.read_csv(\"data/rugby.csv\", index_col=0)\n",
    "except FileNotFoundError:\n",
    "    rugby_data = pd.read_csv(DATA_URL + 'rugby.csv', index_col=0)\n",
    "rugby_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "The league is made up by a total of $T= 6$ teams, playing each other once in a season. We indicate the number of points scored by the home and the away team in the g-th game of the season as $y_{g1}$ and $y_{g2}$ respectively. \n",
    "\n",
    "The vector of observed points $y_{g} = (y_{g1}, y_{g2})$ is modelled as independent Poisson:\n",
    "\n",
    "$$y_{gj} \\sim \\text{Poisson}(\\theta_{gj})$$\n",
    "\n",
    "where the theta parameters represent the scoring intensity in the g-th game for the team playing at home and away, $j=1$ and $j=2$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the Poisson rate is positive, we will build a model using a **log-linear** formulation. The home team's expected score is then given by:\n",
    "\n",
    "$$\\log(\\theta_{g1}) = \\eta + \\alpha_{h[g]} + \\delta_{a[g]}$$\n",
    "\n",
    "The parameter $\\alpha_{h[g]}$ is the attacking skill of the home team in game $g$, while $\\delta_{a[g]}$ is the defensive skill of the away team in the same game.\n",
    "\n",
    "$\\eta$ represents the home advantage, which for simplicity we assume is constant for all teams during the season.\n",
    "\n",
    "To aid interpretaiton, let's reparameterize this model slightly; specifically, we will constrain the team offensive and defensive skills to sum to zero, and include an intercept term that describes the expected score of an average team (excluding the home advantage).\n",
    "\n",
    "$$\\log(\\theta_{g1}) = \\mu + \\eta + \\alpha^*_{h[g]} + \\delta^*_{a[g]}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\alpha^*_t = \\alpha_t - \\bar{\\alpha.}$$\n",
    "\n",
    "$$\\delta^*_t = \\delta_t - \\bar{\\delta.}$$\n",
    "\n",
    "\n",
    "Correspondingly, the away team's expected score is:\n",
    "\n",
    "$$\\log(\\theta_{g2}) = \\mu + \\alpha^*_{a[g]} + \\delta^*_{h[g]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this formulation, we will need indices for all of the teams, in order to extract the appropriate parameters for each game. \n",
    "\n",
    "The pandas `factorize` function is useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_idx, teams = pd.factorize(rugby_data[\"home_team\"], sort=True)\n",
    "away_idx, _ = pd.factorize(rugby_data[\"away_team\"], sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it has been available in later PyMC3 versions, a relatively new feature is the ability to specify variable dimensions using categorical labels. These can be passed to the model in a `dict` called `coords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\"team\": teams}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to generate predictions with our model, we will make our inputs **mutable**. It creates symbolic variables in the model, which can be changed after the model is fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as rugby_model:\n",
    "\n",
    "    home_team = pm.MutableData(\"home_team\", home_idx)\n",
    "    away_team = pm.MutableData(\"away_team\", away_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we specify the priors for the latent variables in our model. This includes the scoring mean, the home team effect, and the vector of team strengths on offense and defense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rugby_model:\n",
    "\n",
    "    eta = pm.Normal(\"eta\", mu=1, sigma=1)\n",
    "\n",
    "    mu = pm.Normal('mu', 3, sigma=1)\n",
    "\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=1, dims=\"team\")\n",
    "    delta = pm.Normal(\"delta\", mu=0, sigma=1, dims=\"team\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will constrain the offense and defense skills to sum to zero, but dividing by the means of each vector of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rugby_model:\n",
    "\n",
    "    alpha_star = pm.Deterministic(\"alpha_star\", alpha - at.mean(alpha), dims=\"team\")  \n",
    "    delta_star = pm.Deterministic(\"delta_star\", delta - at.mean(delta), dims=\"team\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected values for the home and away teams in each game can then be calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rugby_model:\n",
    "\n",
    "    home_theta = at.exp(mu + eta + alpha_star[home_team] + delta[away_team])\n",
    "    away_theta = at.exp(mu + alpha[away_team] + delta[home_team])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the likelhoods of the observed scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rugby_model:\n",
    "\n",
    "    home_score = pm.Poisson(\n",
    "        \"home_score\",\n",
    "        mu=home_theta,\n",
    "        observed=rugby_data[\"home_score\"].values\n",
    "    )\n",
    "    away_score = pm.Poisson(\n",
    "        \"away_score\",\n",
    "        mu=away_theta,\n",
    "        observed=rugby_data[\"away_score\"].values\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Let's take a peek at the graph of the model. This is an easy visual check to ensure that the model has been constructed as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(rugby_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, below is the entire model in a single cell, wrapped in a function so that we can instantiate it as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rugby_model():\n",
    "\n",
    "    with pm.Model(coords=coords) as m:\n",
    "\n",
    "        # Data inputs\n",
    "        home_team = pm.MutableData(\"home_team\", home_idx)\n",
    "        away_team = pm.MutableData(\"away_team\", away_idx)\n",
    "\n",
    "        # Average score (log scale)\n",
    "        mu = pm.Normal('mu', 3, sigma=1)\n",
    "\n",
    "        # Home field effect\n",
    "        eta = pm.Normal(\"eta\", mu=1, sigma=1)\n",
    "\n",
    "        # Team attack and defense skills\n",
    "        alpha = pm.Normal(\"alpha\", mu=0, sigma=1, dims=\"team\")\n",
    "        delta = pm.Normal(\"delta\", mu=0, sigma=1, dims=\"team\")\n",
    "\n",
    "        # Centered skills\n",
    "        alpha_star = pm.Deterministic(\"alpha_star\", alpha - at.mean(alpha), dims=\"team\")  \n",
    "        delta_star = pm.Deterministic(\"delta_star\", delta - at.mean(delta), dims=\"team\")  \n",
    "        \n",
    "        # Expected home and away scores for each game\n",
    "        home_theta = at.exp(mu + eta + alpha_star[home_team] + delta[away_team])\n",
    "        away_theta = at.exp(mu + alpha[away_team] + delta[home_team])\n",
    "\n",
    "        # Likelihood functions\n",
    "        home_score = pm.Poisson(\n",
    "            \"home_score\",\n",
    "            mu=home_theta,\n",
    "            observed=rugby_data[\"home_score\"].values\n",
    "        )\n",
    "        away_score = pm.Poisson(\n",
    "            \"away_score\",\n",
    "            mu=away_theta,\n",
    "            observed=rugby_data[\"away_score\"].values\n",
    "        )\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to fit the model with MCMC.\n",
    "\n",
    "## `sample`\n",
    "\n",
    "The user's interface to PyMC's sampling algorithms is the `sample` function. There is a large set of input parameters, all of which are optional with reasonable defaults, but here are the most common ones that you may want to modify:\n",
    "\n",
    "```python\n",
    "def sample(\n",
    "    draws: int = 1000,\n",
    "    step=None,\n",
    "    init: str = \"auto\",\n",
    "    n_init: int = 200_000,\n",
    "    initvals: Optional[Union[StartDict, \n",
    "        Sequence[Optional[StartDict]]]] = None,\n",
    "    trace: Optional[Union[BaseTrace, List[str]]] = None,\n",
    "    chain_idx: int = 0,\n",
    "    chains: Optional[int] = None,\n",
    "    cores: Optional[int] = None,\n",
    "    tune: int = 1000,\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "`sample` assigns particular samplers to model variables, and generates samples from them. \n",
    "\n",
    "PyMC can automate most of the details of sampling using default settings for several parameters that control how the sampling is set up and conducted. However, users may manually intervene in the specification of the sampling by passing values to a number of keyword argumetns for `sample`.\n",
    "\n",
    "Most commonly, the `draws` argument that \n",
    "controls the total number of MCMC iterations and the `tune` argument that controls the number of additional tuning steps to perform, are modified by the user depending on the model being fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning sampling algorithm\n",
    "\n",
    "The `step` argument allows users to manually assign a MCMC sampling algorithm to the entire model, or to a subset of the variables in the model. For example, if we wanted to use the Metropolis-Hastings sampler to fit our model, we could pass an instance of that step method to `sample` via the `step` argument:\n",
    "\n",
    "```python\n",
    "with rugby_model:\n",
    "\n",
    "    trace = pm.sample(1000, step=pm.Metropolis())\n",
    "```\n",
    "\n",
    "or if we only wanted to assign `Metropolis` to a subset of our parameters:\n",
    "\n",
    "```python\n",
    "with rugby_model:\n",
    "\n",
    "    trace = pm.sample(1000, step=pm.Metropolis(vars=[alpha, delta]))\n",
    "```\n",
    "\n",
    "When `step` is not specified by the user, PyMC will assign step methods to variables automatically. To do so, each step method implements a class method called `Competence`. This method returns a value from 0 (incompatible) to 3 (ideal), based on the attributes of the random variable in question. `sample()` assigns the step method that returns the highest competence value to each of its unallocated stochastic random variables. In general:\n",
    "\n",
    "* Binary variables will be assigned to `BinaryMetropolis` (Metropolis-Hastings for binary values)\n",
    "* Discrete variables will be assigned to `Metropolis`\n",
    "* Continuous variables will be assigned to `NUTS` (No U-turn Sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting values\n",
    "\n",
    "The `initvals` argument allows for the specification of starting values for stochastic random variables in the model. MCMC algorithms begin by initializing all unknown quantities to arbitrary starting values. Though in theory the value can be any value under the support of the distribution describing the random variable, we can make sampling more difficult if an initial value is chosen in the extreme tail of the distribution, for example. \n",
    "\n",
    "If starting values are not passed by the user (`intivals=None`), default values are chosen from the mean, median or mode of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rugby_model.initial_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with create_rugby_model():\n",
    "\n",
    "    initval_trace = pm.sample(draws=500, tune=0, step=pm.Metropolis(), initvals={'eta':-1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ArviZ function `plot_trace` is a useful exploratory tool for examining the resulting samples, which we refer to as a *trace*. On the left is a set of histograms (one for each chain) and on the right is a time series plot of the chains in the order they were drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(initval_trace, var_names=[\"eta\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example was deliberately poor, showing the movement away from the initial values, but each of the chains appear to be stuck in different regions of the parameter space. This model needs tuning!\n",
    "\n",
    "Let's tune the model and generate some results ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rugby_model:\n",
    "\n",
    "    trace = pm.sample(1000, tune=1000, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You likely saw some warnings after running the model as specified above. We will dig into these warnings a bit later.\n",
    "\n",
    "Let's have a peek at some of the parameters of interest. The [ArviZ](https://github.com/arviz-devs/arviz) package allows us to easily plot the posterior estimates. Below is a *forest plot* of the attacking strengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=[\"alpha_star\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on which system you ran the model on, you likely see 2-4 lines per team parameter. These are the posterior highest density interval (HDI, thin line) and interquartile range (thick line), along with the posterior mean (open dot) for each chain. \n",
    "\n",
    "This graphic shows that Italy has the weakest offense, with England having the strongest (though with some overlap with Ireland and Wales).\n",
    "\n",
    "Below is the corresponding plot for the defenses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=[\"delta_star\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For univariate variables, a *posterior plot* can be used to inspect the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(trace, var_names=[\"mu\", \"eta\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, these parameters are on the log scale, so the estimates correspond to an expectation of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(2.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "points for away teams and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(2.8+0.24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "points for home teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing samples\n",
    "\n",
    "Notice in the above call to `sample` that output is assigned to a variable we have called `trace`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `InferenceData` object is a data structure that stores the samples from an MCMC run as grouped attributes. The data structure itself is an `xarray.Dataset` object, which is a dictionary-like object that stores the samples in a multi-dimensional array.\n",
    "\n",
    "The xarray components include:\n",
    "\n",
    "- **Data variables** are the actual values generated from the MCMC draws\n",
    "- **Dimensions** are the axes on which refer to the data variables\n",
    "- **Coordinates** are pointers to specific slices or points in the `xarray.Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel sampling\n",
    "\n",
    "Nearly all modern desktop computers have multiple CPU cores, and running multiple MCMC chains is an **embarrasingly parallel** computing task. It is therefore relatively simple to run chains in parallel in PyMC. This is done by setting the `cores` argument in `sample` to some value between 2 and the number of cores on your machine (you can specify more chains than cores, but you will not gain efficiency by doing so). \n",
    "\n",
    "> Keep in mind that some chains might themselves be multithreaded via openmp or BLAS. In those cases it might be faster to set this to 1.\n",
    "\n",
    "By default, PyMC will run a sample a minimum of 2 and a maximum of `cores` chains. However, the number of chains sampled can be set independently of the number of cores by specifying the `chains` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rugby_model:\n",
    "    ptrace = pm.sample(100, chains=6, cores=4, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running $n$ iterations with $c$ chains will result in $c \\times n$ samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptrace.posterior['alpha'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating several chains is generally recommended because it aids in model checking, allowing statistics such as the potential scale reduction factor ($\\hat{R}$) and effective sample size to be calculated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Checking\n",
    "\n",
    "After running an MCMC simulation, `sample` returns an `arviz.InferenceData` object containing the samples for all the stochastic and named deterministic random variables. The final step in Bayesian computation is model checking, in order to ensure that inferences derived from your sample are valid. There are two components to model checking:\n",
    "\n",
    "1. Convergence diagnostics\n",
    "2. Goodness of fit\n",
    "\n",
    "Convergence diagnostics are intended to detect lack of convergence in the Markov chain Monte Carlo sample; it is used to ensure that you have not halted your sampling too early. However, a converged model is not guaranteed to be a good model. The second component of model checking, goodness of fit, is used to check the internal validity of the model, by comparing predictions from the model to the data used to fit the model. \n",
    "\n",
    "## Convergence Diagnostics\n",
    "\n",
    "### Gazing at Sample Trace\n",
    "\n",
    "The most straightforward approach for assessing convergence is based on\n",
    "simply **plotting and inspecting traces and histograms** of the observed\n",
    "MCMC sample. If the trace of values for any of the stochastic variables displays lack of stationarity or homogeneity, this may be evidence of lack of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with create_rugby_model():\n",
    "\n",
    "    bad_trace = pm.sample(1000, tune=10, chains=2, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(bad_trace, var_names=['eta', 'mu'], backend_kwargs=dict(constrained_layout=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison, here are traces from the full run with 1000 samples after 1000 tuning iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace, var_names=['eta', 'mu'], backend_kwargs=dict(constrained_layout=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divergences\n",
    "\n",
    "During some of the MCMC runs above, you may have noticed warnings about divergnces:\n",
    "\n",
    "    There were 5 divergences after tuning. Increase `target_accept` or reparameterize.\n",
    "\n",
    "You will see these divergences identified in the output of `plot_trace` as black lines along the x-axis.\n",
    "\n",
    "Hamiltonian Monte Carlo (and NUTS) performs numerical integration in order to explore the posterior distribution of a model. When the integration goes wrong, it can go dramatically wrong.\n",
    "\n",
    "For example, here are some Hamiltonian trajectories on the distribution of two correlated variables. Can you spot the divergent path?\n",
    "\n",
    "![](assets/diverging_hmc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason that this happens is that there may be parts of the posterior which are hard to explore for geometric reasons. Two ways of solving divergences are\n",
    "\n",
    "1. **Set a higher `target accept` rate**: Similarly (but not the same) as for Metropolis-Hastings, larger integrator steps lead to lower acceptance rates. A higher `target_accept` will generally cause a smaller step size, and more accurate integration, but result in longer run times.\n",
    "2. **Reparametrize**: If you can write your model in a different way that has the same joint probability density, you might do that. A lot of work is being done to automate this, since it requires careful work, and one goal of a probabilistic programming language is to iterate quickly. \n",
    "   \n",
    "   \n",
    "You should be wary of a trace that contains many divergences (particularly those clustered in particular regions of the parameter space), and attempt to fix them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Fraction of Missing Information\n",
    "\n",
    "The Bayesian fraction of missing information (BFMI) is a measure of how hard it is to\n",
    "sample level sets of the posterior at each iteration. Specifically, it quantifies **how well momentum resampling matches the marginal energy distribution**. \n",
    "\n",
    "$$\\text{BFMI} = \\frac{\\mathbb{E}_{\\pi}[\\text{Var}_{\\pi_{E|q}}(E|q)]}{\\text{Var}_{\\pi_{E}}(E)}$$\n",
    "\n",
    "$$\\widehat{\\text{BFMI}} = \\frac{\\sum_{i=1}^N (E_n - E_{n-1})^2}{\\sum_{i=1}^N (E_n - \\bar{E})^2}$$\n",
    "\n",
    "A small value indicates that the adaptation phase of the sampler was unsuccessful, and invoking the central limit theorem may not be valid. It indicates whether the sampler is able to *efficiently* explore the posterior distribution.\n",
    "\n",
    "Though there is not an established rule of thumb for an adequate threshold, values close to one are optimal. Reparameterizing the model is sometimes helpful for improving this statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.bfmi(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of diagnosting this phenomenon is by comparing the *overall distribution of energy levels* with the *change of energy between successive samples*. Ideally, they should be very similar.\n",
    "\n",
    "If the distribution of energy transitions is narrow relative to the marginal energy distribution, this is a sign of inefficient sampling, as many transitions are required to completely explore the posterior. On the other hand, if the energy transition distribution is similar to that of the marginal energy, this is evidence of efficient sampling, resulting in near-independent samples from the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad energy plots are obvious, with the marginal energy distribution being much broader than the energy transition distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(bad_trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Scale Reduction: $\\hat{R}$\n",
    "\n",
    "Roughly, $\\hat{R}$ (*R-Hat*, or the *potential scale reduction factor*) is the ratio of between-chain variance to within-chain variance. This diagnostic uses multiple chains to\n",
    "check for lack of convergence, and is based on the notion that if\n",
    "multiple chains have converged, by definition they should appear very\n",
    "similar to one another; if not, one or more of the chains has failed to\n",
    "converge.\n",
    "\n",
    "$\\hat{R}$ uses an analysis of variance approach to\n",
    "assessing convergence. That is, it calculates both the between-chain\n",
    "varaince (B) and within-chain varaince (W), and assesses whether they\n",
    "are different enough to worry about convergence. Assuming $m$ chains,\n",
    "each of length $n$, quantities are calculated by:\n",
    "\n",
    "$$\\begin{align}B &= \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{\\theta}_{.j} - \\bar{\\theta}_{..})^2 \\\\\n",
    "W &= \\frac{1}{m} \\sum_{j=1}^m \\left[ \\frac{1}{n-1} \\sum_{i=1}^n (\\theta_{ij} - \\bar{\\theta}_{.j})^2 \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "for each scalar estimand $\\theta$. Using these values, an estimate of\n",
    "the marginal posterior variance of $\\theta$ can be calculated:\n",
    "\n",
    "$$\\hat{\\text{Var}}(\\theta | y) = \\frac{n-1}{n} W + \\frac{1}{n} B$$\n",
    "\n",
    "Assuming $\\theta$ was initialized to arbitrary starting points in each\n",
    "chain, this quantity will overestimate the true marginal posterior\n",
    "variance. At the same time, $W$ will tend to underestimate the\n",
    "within-chain variance early in the sampling run. However, in the limit\n",
    "as $n \\rightarrow \n",
    "\\infty$, both quantities will converge to the true variance of $\\theta$.\n",
    "In light of this, $\\hat{R}$ monitors convergence using\n",
    "the ratio:\n",
    "\n",
    "$$\\hat{R} = \\sqrt{\\frac{\\hat{\\text{Var}}(\\theta | y)}{W}}$$\n",
    "\n",
    "This is called the **potential scale reduction**, since it is an estimate of\n",
    "the potential reduction in the scale of $\\theta$ as the number of\n",
    "simulations tends to infinity. In practice, we look for values of\n",
    "$\\hat{R}$ close to one (say, less than 1.1) to be confident that a\n",
    "particular estimand has converged. \n",
    "\n",
    "In ArViZ, the `summary` table, or a `plot_forest` with the `r_hat` flag set, will calculate $\\hat{R}$ for each stochastic node in the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Goodness of Fit\n",
    "\n",
    "As noted at the outset, convergence diagnostics are only the first step in the evaluation\n",
    "of MCMC model outputs. It is possible for an entirely unsuitable model to converge to something, so additional steps are needed to ensure that the estimated model adequately fits the data. \n",
    "\n",
    "One intuitive way of evaluating model fit is to compare model predictions with the observations used to fit\n",
    "the model. In other words, the fitted model can be used to simulate data, and the distribution of the simulated data should resemble the distribution of the actual data.\n",
    "\n",
    "Fortunately, simulating data from the model is a natural component of the Bayesian modelling framework. Recall, from the discussion on prediction, the posterior predictive distribution:\n",
    "\n",
    "$$p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta) f(\\theta|y) d\\theta$$\n",
    "\n",
    "Here, $\\tilde{y}$ represents some hypothetical new data that would be expected, taking into account the posterior uncertainty in the model parameters. \n",
    "\n",
    "Sampling from the posterior predictive distribution is easy in PyMC. The `sample_posterior_predictive` function draws posterior predictive samples from all of the observed variables in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior predictive distribution of rugby scores uses the same functional form as the data likelihood, in this case a Poisson stochastic. Here is the corresponding sample from the posterior predictive distribution (we typically need very few samples relative to the MCMC sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rugby_model:\n",
    "\n",
    "    ppc = pm.sample_posterior_predictive(trace, extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates draws that account both for the residual posterior uncertainty of the model parameters and the stochastic variability of the sampling distribution.\n",
    "\n",
    "Setting the `extend_inferencedata` flag will append the resulting samples to the trace that is passed in, adding a `posterior_predictive` node to the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easiest to evaluate the calibration of the model visually, using the `plot_ppc` function. This plots the posterior predictive samples (and the mean prediction) relative to the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(trace, kind='cumulative');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Now that we are (mostly) happy with the state of the fitted model, we can use it to make predictions.\n",
    "\n",
    "If you recall from the original model specification, we created `MutableData` variables for the input data of home and away teams for each of the matches played. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(rugby_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these variables are nodes in the underlying Aesara graph, we can swap in new data for for matchups we wish to predict. The `set_data` function allows us to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_team = 'England'\n",
    "away_team = 'Wales'\n",
    "\n",
    "with rugby_model:\n",
    "\n",
    "    pm.set_data({\n",
    "        \"home_team\": np.where(teams == home_team)[0],\n",
    "        \"away_team\": np.where(teams == away_team)[0],\n",
    "    })\n",
    "\n",
    "    pred = pm.sample_posterior_predictive(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "away_pred = pred.posterior_predictive['away_score'].values.flatten()\n",
    "home_pred = pred.posterior_predictive['home_score'].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(away_pred, label=f\"{away_team} (away)\");\n",
    "sns.kdeplot(home_pred, label=f\"{home_team} (home)\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(home_pred > away_pred).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## To Learn More\n",
    "\n",
    "- **Peadar Coyle**'s [A Hierarchical Model for Rugby Prediction](https://docs.pymc.io/en/v3/pymc-examples/examples/case_studies/rugby_analytics.html) example notebook on the PyMC website.\n",
    "- **Michael Betancourt**'s [A Conceptual Introduction to Hamiltonian Monte Carlo](https://arxiv.org/abs/1701.02434)\n",
    "- **Hoffman, M. D., and A. Gelman. (2014)**. The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research: JMLR 15 (1): 1593–1623.\n",
    "- [Hamiltonian Monte Carlo from Scratch](https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/), by **Colin Carroll**\n",
    "- **Gelman, A., & Rubin, D. B.** (1992). Inference from iterative simulation using multiple sequences. Statistical Science. A Review Journal of the Institute of Mathematical Statistics, 457–472.\n",
    "- **Abril, O.** (2019). [LOO-PIT Tutorial](https://oriolabril.github.io/oriol_unraveled/python/arviz/pymc3/2019/07/31/loo-pit-tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new course from [PyMC Labs](https://pymc-labs.io)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/inutitive_bayes.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "125ae30d70f5add3b55791599cf15605bb1e3472d016e70897800267b063dac3"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('prob_python')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nteract": {
   "version": "0.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
